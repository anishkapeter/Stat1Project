---
title: "HousePrice Prediction"
output:
  html_document:
    toc: yes
    df_print: paged
  html_notebook:
    toc: yes
date: "2023-07-30"
---

# Load necessary packages, load data.

```{R,warning=FALSE, message=FALSE}
#load necessary packages:
library(car)
library(tidyverse)
library(caret)
library(multiUS)
library(boot)
library(ggplot2)
library(ggpubr)

#Load Data

data=read.csv('https://raw.githubusercontent.com/anishkapeter/Stat1Project/main/train.csv')

```
# Fill missing values in numeric columns with KNN imputation
```{r,warning=FALSE, message=FALSE}
numeric_cols <- names(data)[sapply(data, is.numeric)]
#colSums(is.na(data[,numeric_cols]))
#mode(data$LotFrontage)


categorical_cols <- names(data)[sapply(data, is.character)]
#length(categorical_cols)+length(numeric_cols)
numeric_data <- data[, numeric_cols]
numeric_data_imputed <- KNNimp(as.matrix(numeric_data))  # convert data frame to matrix for knnImpute
data[numeric_cols] <- numeric_data_imputed

```

# Define ordinal columns and apply factor encoding
```{r,warning=FALSE, message=FALSE}
ordinal_cols <- c('ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 
                  'GarageCond', 'GarageQual', 'PoolQC', 'FireplaceQu', 
                  'HeatingQC', 'KitchenQual', 'BsmtExposure', 'BsmtFinType1', 
                  'BsmtFinType2', 'CentralAir', 'Fence','GarageFinish','Alley','LotShape','LandSlope')



data[ordinal_cols] <- lapply(data[ordinal_cols], function(x) as.numeric(as.factor(x)))

data[ordinal_cols] <- lapply(data[ordinal_cols], function(x) replace(x, is.na(x), 0))





# Transform ordinal columns into factors and replace NA with 0

#data[ordinal_cols] <- lapply(data[ordinal_cols], function(x) {
#  x <- as.factor(x)
#  levels(x) <- c(levels(x), "0")
# x[is.na(x)] <- "0"
#  return(x)
# })

# Convert all character columns to factors and add NA as a level
colnames(data)[sapply(data, is.character)]



# Threshold for 'rare' observations
n <- 10 

# Loop through the character columns
for (col_name in colnames(data)[sapply(data, is.character)]) {
  # Convert rare observations to 'Other'
  data[[col_name]] <- ifelse(data[[col_name]] %in% names(which(table(data[[col_name]]) < n)), "Other", data[[col_name]])
  
  # Convert column to factor
  data[[col_name]] <- as.factor(data[[col_name]])

  # Add NA as a level if there are any NAs in the column
  if(any(is.na(data[[col_name]]))) {
    data[[col_name]] <- addNA(data[[col_name]])
    
  
  }
}

#Deleting some columns
rare_level_factors <- sapply(data2, function(x) is.factor(x) && any(table(x) <= 2))
print(names(data2)[rare_level_factors])


data$Electrical=as.character(data$Electrical)
data$Electrical[is.na(data$Electrical)] <- "Other"
data$Electrical=as.factor(data$Electrical)
table(data$Electrical)

data <- data[,-which(names(data) == "Utilities")]


# No more missing values, ordinary variable has been factorized

sum(is.na(data))
#class(data$MiscFeature)
#table(data$Utilities)




```


# Checking assumption for regression (full model)

```{r,warning=FALSE, message=FALSE}
model = lm(SalePrice ~., data = data)


# residual plots / Histogram / Q-Q plot / Calculate Cook's distances /Create Residuals vs Leverage plot
histogram(model$residuals)
plot(model)



#Look for influential observations

#Looking at observation 1171,1424, delete them in the data
data= data[-c(1171,1424),]
```




# Loggin SalePrice to see if it fix the clusterred residuals
```{r,warning=FALSE, message=FALSE}
data2=data
data2$SalePrice = log(data$SalePrice)
model2 = glm(SalePrice~., data = data2)
residualPlot(model2)


# Compare to model1(unlogged) , the residual plots forms a rather perfect random cloud formation which is ideal
# Histogram
res2= model2$residuals
histogram(res2)

plot(model2)
```



# Perform a forward/backward/stepwise selection based on AIC 

```{r,results='hide',warning=FALSE, message=FALSE}
null_model <- glm(SalePrice ~ 1, data = data2)
forward_model <- step(null_model, 
                      scope = list(lower = null_model, upper = model2), 
                      direction = "forward")

# Perform backward selection based on AIC
backward_model <- step(model2, 
                       direction = "backward")


# Perform stepwise selection
stepwise_model <- step(model2, direction = "both")
```



# Calculating CV Press 
```{r,warning=FALSE, message=FALSE}







cv_errors <- numeric(nrow(data2))

for(i in 1:nrow(data2)) {
  model_loocv <- glm(formula(forward_model), 
    data = data2[-i, ])
  predicted <- predict(model_loocv, newdata = data2[i, ])
  cv_errors[i] <- sum((predicted - data2$SalePrice[i])^2)
}
cv_error <- mean(cv_errors, na.rm = TRUE)





# Forward CV Press
forward <- glm(formula(forward_model), data = data2)


cv.error_foward <- cv.glm(data2, forward, K = 10)

cv.error_foward$delta



# Backward CV Press
Backward <- glm(formula(backward_model), data = data2)


cv.error_Backward <- cv.glm(data2, Backward, K = 10)

cv.error_Backward$delta


# Stepwise CV Press
Stepwise <- glm(formula(stepwise_model), data = data2)


cv.error_Stepwise<- cv.glm(data2, Stepwise, K = 10)

cv.error_Stepwise$delta



```



# Build custom regression model





# Examen relationship in Sales price in neighborhood 'NAmes', 'Edwards', 'BrkSide' 
```{r,warning=FALSE, message=FALSE}
options(scipen = 999)
library(dplyr)
library(ggplot2)
data= read.csv("https://raw.githubusercontent.com/anishkapeter/Stat1Project/main/train.csv")
# Model without Interaction Variable 
filtered_neighborhood = data %>% filter (Neighborhood %in% c('NAmes', 'Edwards', 'BrkSide'))

filtered_neighborhood_model= lm(SalePrice ~ GrLivArea, data = filtered_neighborhood)
summary(filtered_neighborhood_model)

# Check Assumptions Model Without Indicator 
residualPlot(filtered_neighborhood_model)
histogram(filtered_neighborhood_model$residuals)
ggplot(filtered_neighborhood,aes(x =GrLivArea , y =SalePrice )) + 
  geom_point() +
  ggtitle("ScatterPlot of SalePrice vs GrLivArea") 

# Residual plot clustered, using log-log method to transform the dataset

# Log log Transform to data 
data3=data
data3$SalePrice = log(data$SalePrice)
data3$GrLivArea = log(data$GrLivArea)

# filter the logged data to only include the 3 neighborhoods of interest
filtered_data2 <- data3 %>% filter(Neighborhood %in% c('NAmes', 'Edwards', 'BrkSide'))

# build model on log data without interaction 
filtered_data2_model = lm(SalePrice ~ GrLivArea, data = filtered_data2)

# Checking Assumptions 
hist(filtered_data2_model$residuals)
plot(filtered_data2_model)
ggplot(filtered_data2,aes(x =GrLivArea , y =SalePrice )) +
  geom_point()+ggtitle("ScatterPlot of Log(SalePrice) vs Log(GrLivArea)") + 
  xlab("Log(GrLivArea)") +
  ylab("Log(SalePrice)")


# Confidence Interval for Log Model without Interaction 
confint(filtered_data2_model)

plot(filtered_data2$GrLivArea, filtered_data2$SalePrice, 
     main="Scatterplot with Regression Line", 
     xlab="Above grade (ground) living area square feet (GrLivArea)", 
     ylab="Sale Price (SalePrice)", pch=19, frame=FALSE, col="blue")+
  abline(filtered_data2_model, col="red")


```
#### Therefore the regression model predicting SalePrice using GrlivArea is 
SalePrice = 7.58437 + 0.59230 * GrlivArea

# Adding Indicator Variable 
```{r}
filtered_data2_model3 <- lm(SalePrice ~  GrLivArea + Neighborhood, data = filtered_data2)
summary(filtered_data2_model3)
```


# Adding interaction variables
```{r, warning=FALSE, message=FALSE}

# Fit the model
filtered_data2_model2 <- lm(SalePrice ~  GrLivArea + GrLivArea * Neighborhood, data = filtered_data2)

# Summary of the model
summary(filtered_data2_model2)

# Confidence intervals
confint(filtered_data2_model2)


# Residuals histogram
histogram <- ggplot(filtered_data2_model2, aes(x=residuals)) +
  geom_histogram(binwidth=1, color="black", fill="white") +
  theme_minimal() +
  labs(x="Residuals", y="Frequency", title="Histogram of Residuals")

# Model diagnostics
plot_diag <- plot(filtered_data2_model2, diagnostic = TRUE)


# Model Coefficients
model_coef <- coef(filtered_data2_model2)
```
## Anova Analysis 
```{r}
anova(filtered_data2_model3)
anova(filtered_data2_model2)
anova(filtered_data2_model)
1 - pf(68.02972973,2,377)

1 - pf(8.641891892,2,377)


```


# Model with Interaction Summaries 

#### For each neighborhood, the regression model predicting SalePrice using GrLivArea is given by:

#### Neighborhood 'BrkSide':

logSalePrice =  5.91292 + 0.81965*logGrLivArea

##### Interpretation of the Coefficients 
Every time the square footage of the living area doubles, there is an estimated multiplicative increase of 1.76497775436 (about 76.5% increase) in the median Sale Price.  

When the square footage of the living area is 0, the estimated median SalePrice 369.78435. 

##### Interpretation of Confidence Intervals   
Every time the square footage of the living area doubles, the estimated multiplicative increase in median Sales Price for Brookeside is between 1.60081478829 and 1.94597031156. 

When the square footage of the living area is 0., the estimated median Sale Price is between 137.10639085 and 997.332584908 in Brookside neighborhood.  


#### Neighborhood 'NAmes':
logSalePrice = 7.74784 + 0.47303*logGrLivArea


##### Interpretation of the Coefficients 
Every time the square footage of the living area doubles, there is an estimated multiplicative increase of 1.38802158181, (about 38.8% increase) in the median Sale Price.  

When the square footage of the living area is 0, the estimated median SalePrice 4879.16803655. 

##### Interpretation of Confidence Intervals   
When the square footage of the living area is 0, the estimated median SalePrice 4879.16803655 . 

When the square footage of the living area is 0, the estimated median Sale Price is between 556.146417272 and 42805.5779923 in North Ames neighborhood. 
 


#### Neighborhood 'Edwards':
logSalePrice = 8.00651 + 0.51967 *logGrLivArea

##### Interpretation of the Coefficients 
Every time the square footage of the living area doubles, there is an estimated multiplicative increase of 1.4336074105 (about 43.36% increase) in the median Sale Price.  

When the square footage of the living area is 0, the estimated median Sale Price 3000.42732748.  

##### Confidence Intervals and Interpretation 

When the square footage of the living area is 0, the estimated median Sale Price is between 312.416333335 and 28815.756004 for houses in the Edwards Neighborhood. 

Every time the square footage of the living area doubles, the estimated multiplicative increase in median Sales Price for Edwards Neighborhood is between 1.14827731299 and 1.78988066094. 






filtered_data2
library(ggplot2)
filtered_neighborhood %>% 
  filter(Neighborhood == "BrkSide") %>% 
  ggplot(aes(x=GrLivArea, y = SalePrice)) + 
  geom_point( color = "steelblue") + 
  ggtitle("Sale Price vs Living Area Sq.Ft in Brookside") +
  xlab("Square Footage of Living Area") +
  ylab("Sales Price in Dollars")


filtered_neighborhood %>% 
  filter(Neighborhood == "Edwards") %>% 
  ggplot(aes(x=GrLivArea, y = SalePrice)) + 
  geom_point( color = "steelblue") + 
  ggtitle("Sale Price vs Living Area Sq.Ft in Edwards") +
  xlab("Square Footage of Living Area") +
  ylab("Sales Price in Dollars")

filtered_neighborhood %>% 
  filter(Neighborhood == "NAmes") %>% 
  ggplot(aes(x=GrLivArea, y = SalePrice)) + 
  geom_point(color = "steelblue") + 
  ggtitle("Sale Price vs Living Area Sq.Ft in North Ames") +
  xlab("Square Footage of Living Area") +
  ylab("Sales Price in Dollars")






